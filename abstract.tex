\begin{foreignabstract}

In the literature of Recommender Systems, Active Learning strategies have been extensively applied to rating elicitation in order to mitigate Cold Start effects, namely \textit{Rating Elicitation for Cold Start Purposes}. However, if we knew the best $N$ items, among those already acquired but not evaluated, which, if evaluated, would result in the greatest improvement in terms of overall system's accuracy, it would probably be worthwhile to give users an incentive for evaluating those items. We call this task \textit{Rating Elicitation for Incentives Purposes}. 

This work proposes a novel Active Learning strategy that selects items based on their probability distribution, creating an unbiased training set. The so called \textit{Unbiased Strategy} was compared with other 16 popular strategies in the literature concerning \textit{Rating Elicitation for Incentives Purposes}. The \textit{Unbiased Strategy} outperformed  the others in terms of overall system's accuracy. Moreover, we present an analysis of each strategy's performance, taking into account the possible reasons for their success or failure.

\end{foreignabstract}

