\chapter{Conclusões e Trabalhos Futuros}
\label{cap:conclusao}

Há três resultados importantes em nossos experimentos que merecem atenção. O primeiro é o bom desempenho obtido por \textit{random} ou, olhando sob outro aspecto, o desempenho ruim obtido pela maioria das estratégias, com exceção de \textit{low pred} e \textit{high-low pred}. As heurísticas que formulam as estratégias ditam a maneira como os itens serão selecionados para inclusão no conjunto de treinamento. Por exemplo, \textit{entropy}, \textit{variance} e suas derivadas assumem que os melhores itens a serem incluídos no conjunto de treinamento são aqueles mais controversos; \textit{igcn} assume que os melhores itens são aqueles que melhor distribuem os usuários em \textit{clusters}; \textit{high pred} assume que o melhor conjunto de treinamento é aquele formado por itens com as maiores avaliações; enquanto que \textit{bin pred} assume que o melhor conjunto é aquele formado por itens com grande chance de serem consumidos.

Em todos esses casos, há uma suposição, ou heurística, que guia a seleção dos itens, gerando assim um conjunto formado por itens que satisfazem esta suposição e não necessariamente por aqueles que melhor representam a população de itens. Seria correto afirmar então que o emprego de estratégias de AA quase sempre acarreta em conjuntos de treinamento enviesados, o que, à primeira vista, pode parecer indesejado. No entanto, ter um conjunto de treinamento enviesado nem sempre é algo ruim. Nosso segundo resultado digno de ser destacado é o fato de que \textit{low pred} e \textit{high-low pred} obtiveram bons resultados mesmo formando conjuntos de treinamento enviesados. Ou seja, há casos onde um conjunto enviesado pode ser útil. 

Em nossos experimentos, como as preferências 3 e 4 excedem as demais, um conjunto equilibrado promoveu um bom desempenho do modelo, apesar de ser, tecnicamente, enviesado. Todavia, em situações reais muitas vezes não se conhece as características da base de dados, sem mencionar que as mesmas estão em constante mutação conforme a base cresce. Mesmo em um ambiente isolado e controlado como o nosso, foi necessário testar várias estratégias, baseadas em diversas heurísticas diferentes, para encontrarmos aquela que é adequada aos dados (apenas duas heurísticas se mostraram adequadas). Efetuar tal comparação em um sistema real seria muito complexo, sem contar que o resultado seria provisório dado o carácter volátil de um SR. Ou seja, os projetistas de SR necessitam de uma estratégia que possa ser utilizada em qualquer ocasião, sejam quais forem as características da base de dados e, de preferência, independentemente do modelo adotado.

Isto nos leva ao terceiro resultado que é o excelente desempenho obtido pela \textit{Estratégia Livre de Viés}. Tal estratégia assume que o melhor conjunto de treinamento é simplesmente aquele que melhor representa a população, ou seja, aquele onde não há viés. Nossos experimentos mostraram que a \textit{Estratégia Livre de Viés} de fato promove o bom desempenho do modelo, superando às demais. Embora ainda desejamos realizar mais testes, em outras bases, acreditamos ter encontrado uma excelente opção, dentre as diversas estratégias da literatura, que pode ser empregada em qualquer ocasião, sob quaisquer circunstâncias e com qualquer modelo de recomendação. Assim, a \textit{Estratégia Livre Viés} é uma ótima opção para um SR, visto que o risco relacionado à adoção de outra estratégia, cuja heurística pode não condizer com os dados, é alto.

Como trabalhos futuros, além de analisar o comportamento da \textit{Estratégia Livre de Viés} com bases maiores, pretendemos analisá-la sob situações reais. O problema de \textit{Elicitação de Preferências para fins de Incentivo} possui muitas nuances que só podem ser examinadas em um ambiente \textit{online}. Para tal, desejamos utilizar a plataforma \textit{Amazon Mechanical Turk} \citep{lee2013alleviating} onde seria possível remunerar usuários em troca de suas avaliações. A partir de então poderíamos tentar responder perguntas do tipo: ``Como saber se o usuário está dando sua verdadeira preferência ou se está avaliando aleatoriamente apenas para receber o incentivo?''; ``Quanto se deve investir em incentivos para se ter ganhos significativos na acurácia das recomendações?''; ``Qual o valor mínimo de incentivo para motivar um usuário a dar sua preferência?''; ``Vale a pena um sistema investir em incentivos em troca de avaliações?''. Acreditamos que essas questões são, na atualidade, as questões mais cruciais na área de SR.

Quanto a aspectos teóricos, há casos onde um conjunto de treinamento equilibrado pode ser mais benéfico que um conjunto de treinamento sem viés. Por exemplo, em problemas de classificação onde uma das classes é muita rara, é necessário construir um conjunto de treinamento balanceado, senão correremos o risco do modelo nunca conseguir prever instâncias da classe rara. Neste caso, o conjunto balanceado é enviesado, porém é a maneira mais eficaz de se treinar o modelo. Portanto, pretendemos estudar quando e sob quais circunstâncias vale a pena optar pelo conjunto enviesado em detrimento do mais representativo e vice-versa, procurando estabelecer um limiar teórico com base na proporção das classes.





